# <ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ>

import requests
import json
import time
import sys
from collections import namedtuple, OrderedDict
import xml.etree.ElementTree as ET
import re
from urllib import parse
import pandas as pd
import urllib.request
from soynlp.normalizer import *



# <ë„¤ì´ë²„ ë§ì¶¤ë²• ê²€ì‚¬ê¸°>
# ì„¤ëª… : 1) ì˜¤íƒ€ ìˆ˜ì •, 2) ë„ì–´ì“°ê¸° ìˆ˜ì •

class Naver:
    base_url = 'https://m.search.naver.com/p/csearch/ocontent/util/SpellerProxy'
    _agent = requests.Session()
    PY3 = sys.version_info[0] == 3

    class CheckResult:
        PASSED = 0
        WRONG_SPELLING = 1
        WRONG_SPACING = 2
        AMBIGUOUS = 3
        STATISTICAL_CORRECTION = 4

    _Checked = namedtuple('Checked', ['result', 'original', 'checked', 'errors', 'words', 'time'])

    def __init__(self):
        self.token = self.read_token()

    def read_token(self):
        try:
            with open("token.txt", "r") as f:
                return f.read()
        except:
            return "-"

    def update_token(self):
        html = self._agent.get(url='https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=ë§ì¶¤ë²•ê²€ì‚¬ê¸°')
        match = re.search('passportKey=([a-zA-Z0-9]+)', html.text)
        if match is not None:
            self.token = parse.unquote(match.group(1))
            with open("token.txt", "w") as f:
                f.write(self.token)

    def _remove_tags(self, text):
        text = u"<content>{}</content>".format(text).replace("<br>", "\n")
        if not self.PY3:
            text = text.encode("utf-8")
        return "".join(ET.fromstring(text).itertext())

    def _get_data(self, text):
        payload = {
            "_callback": "window.__jindo2_callback._spellingCheck_0",
            "q": text,
            "color_blindness": 0,
            "passportKey": self.token
        }
        headers = {
            "Host": "m.search.naver.com",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.9 Safari/537.36",
            "referer": "https://search.naver.com/",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept": "*/*"
        }
        start_time = time.time()
        response = self._agent.get(self.base_url, params=payload, headers=headers)
        passed_time = time.time() - start_time
        response_data = response.text[42:-2]
        return passed_time, json.loads(response_data)

# ë„¤ì´ë²„ ë§ì¶¤ë²• ê²€ì‚¬ì— ì‚¬ìš©í•  í•¨ìˆ˜
    def convert_spelling(self, text):
        if isinstance(text, list):
            return [self.convert_spelling(item) for item in text]

        if len(text) > 500:
            return self._Checked(result=False, original=text, checked='', errors=0, words=[], time=0.0)

        passed_time, data = self._get_data(text)
        if "error" in data["message"]:
            self.update_token()
            passed_time, data = self._get_data(text)
            if "error" in data["message"]:
                return self._Checked(result=False, original=text, checked='', errors=0, words=[], time=0.0)

        html = data["message"]["result"]["html"]
        checked_text = self._remove_tags(html)
        errors = data["message"]["result"]["errata_count"]
        words = self._process_words(html)
        return self._Checked(result=True, original=text, checked=checked_text, errors=errors, words=words, time=passed_time)



# ë„¤ì´ë²„ ë§ì¶¤ë²• ê²€ì‚¬ì— ì‚¬ìš©í•  í•¨ìˆ˜ë¥¼
# ìš°ë¦¬ ì¡°ì˜ Taskì— ë§ëŠ” ì „ì²˜ë¦¬ ë°©ë²•ìœ¼ë¡œ ìˆ˜ì •í•œ í•¨ìˆ˜ 
    def Convert_spelling(self, sentences):
        sentences = list(sentences)

        result = []
        for i, sentence in enumerate(sentences):

            if i % 100 == 0:
                print(f"{i}ë²ˆì§¸ Index ì§„í–‰ì¤‘")
                time.sleep(1)            
            
            try:
                checked_sentence = self.convert_spelling(sentence).checked
                result.append(checked_sentence)

            except Exception as e:
                print(f"Error: {e}", f"/ ì„¤ëª… : {i}ë²ˆì§¸ Indexì—ì„œ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤. 5ì´ˆ í›„, {i+1}ë²ˆì§¸ Indexë¶€í„° ë‹¤ì‹œ ì‹œì‘í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤!")
                result.append(sentence)
                time.sleep(5)
                continue

        
        return pd.Series(result)



    def _process_words(self, html):
        words_dict = OrderedDict()
        html = html.replace("<span class=\"green_text\">", "<green>") \
                   .replace("<span class=\"red_text\">", "<red>") \
                   .replace("<span class=\"purple_text\">", "<purple>") \
                   .replace("<span class=\"blue_text\">", "<blue>") \
                   .replace("</span>", "<end>")
        items = html.split(" ")
        temp_tag = ""
        for word in items:
            if temp_tag == "" and word[:1] == "<":
                pos = word.find(">") + 1
                temp_tag = word[:pos]
            elif temp_tag != "":
                word = f"{temp_tag}{word}"

            if word.endswith("<end>"):
                word = word.replace("<end>", "")
                temp_tag = ""

            check_result = self.CheckResult.PASSED
            if word.startswith("<red>"):
                check_result = self.CheckResult.WRONG_SPELLING
                word = word.replace("<red>", "")
            elif word.startswith("<green>"):
                check_result = self.CheckResult.WRONG_SPACING
                word = word.replace("<green>", "")
            elif word.startswith("<purple>"):
                check_result = self.CheckResult.AMBIGUOUS
                word = word.replace("<purple>", "")
            elif word.startswith("<blue>"):
                check_result = self.CheckResult.STATISTICAL_CORRECTION
                word = word.replace("<blue>", "")
            words_dict[word] = check_result

        return words_dict



class NLP_Preprocessor:

# ì •ì œ

    # í•¨ìˆ˜ : ì˜¤íƒ€ ìˆ˜ì • (ì±„íƒ)
    def delete_welcome(self, context):
        preprocessed_text = []
        for text in context:
          text = re.sub(r'ã„±ã…Š', 'ê´œì°®', text).strip()
          text = re.sub(r'ë¯¸ì¶‹ì–´', 'ë¯¸ì³¤ì–´', text).strip()
          text = re.sub(r'ì ¹ë‹¹', 'ì ë‹¹', text).strip()
          text = re.sub(r'ëŒ±', 'ë‹¤', text).strip()
          text = re.sub(r'ëƒ¤ìŠµ', 'ë‚¬ìŠµ', text).strip()
          text = re.sub(r'ê´œì¶˜|ê´œì¸¦|ê´œì¶”ë‹ˆ|ê´œíƒ†|ê´˜ì•Š', 'ê´œì°®', text).strip()
          text = re.sub(r'í›„ì¦', 'í›„ì¤„', text).strip()
          text = re.sub(r'ê¸‰ë‹ˆ|ìŠºë‹ˆ', 'ìŠµë‹ˆ', text).strip()
          text = re.sub(r'ëŠ¨', 'ëŠ”', text).strip()
          text = re.sub(r'ëŠê»¨', 'ëŠë‚Œ', text).strip()
          text = re.sub(r'ì§„ì§²', 'ì§„ì§œ', text).strip()
          text = re.sub(r'ì •ë§', 'ì •ë§', text).strip()
          text = re.sub(r'ì¤³', 'ì¤¬', text).strip()
          text = re.sub(r'ê·€ì—¼|ê¸°ìš¤|ê·€ì–|ê·€ìš¤', 'ê·€ì—¬ì›€', text).strip()
          text = re.sub(r'ì¬ë”œ', 'ì¬ì§ˆ', text).strip()
          text = re.sub(r'í˜ë”©', 'íŒ¨ë”©', text).strip()
          text = re.sub(r'ë»¡ë»', 'ë½€ë½€', text).strip()
          text = re.sub(r'í¬ìŠíŠ¸', 'í¬ì¸íŠ¸', text).strip()
          text = re.sub(r'ìš¬|ìš¯', 'ìš”', text).strip()
          text = re.sub(r'ìœµ', 'ìš©', text).strip()
          text = re.sub(r'ëŒ±', 'ë‹¹', text).strip()
          text = re.sub(r'ì…ì—ìˆ˜', 'ì…ì„ìˆ˜', text).strip()
          text = re.sub(r'ì™„ì ˜', 'ì™„ì „', text).strip()
          text = re.sub(r'ë“·', 'ë“¯', text).strip()
          text = re.sub(r'ì‚¿', 'ìƒ€', text).strip()
          text = re.sub(r'ìŠº', 'ìŠµ', text).strip()
          text = re.sub(r'ë”°ë‘£|ë”°ëš¯|ëœ¨ëœ»', 'ë”°ëœ»', text).strip()
          text = re.sub(r'ìœ½ì‹œ', 'ì—­ì‹œ', text).strip()
          text = re.sub(r'ì¥¼', 'ì¤Œ', text).strip()
          text = re.sub(r'í–¤', 'í–ˆ', text).strip()
          text = re.sub(r'ì—', 'ì„', text).strip()
          text = re.sub(r'ì™„ì¡´', 'ì™„ì „', text).strip()
          text = re.sub(r'ëŸˆ', 'ë', text).strip()
          text = re.sub(r'ì©—', 'ì§§', text).strip()
          text = re.sub(r'ì ›', 'ì¢‹', text).strip()
          text = re.sub(r'ê·£|ê·¯', 'êµ¿', text).strip()
          text = re.sub(r'ì£µ|ì£»', 'ì£ ', text).strip()
          text = re.sub(r'ì•–|ì›‚|ì˜¶', 'ì—†', text).strip()
          text = re.sub(r'ì¥¬ì•„', 'ì¢‹ì•„', text).strip()
          text = re.sub(r'ì¡°ìŒ', 'ì¢‹ìŒ', text).strip()
          text = re.sub(r'ì˜ˆë¿', 'ì˜ˆì©', text).strip()
          text = re.sub(r'ì´ë½€|ì• ì˜|ì—ì˜', 'ì˜ˆì˜', text).strip()
          text = re.sub(r'ì•†', 'ì•˜', text).strip()
          text = re.sub(r'ì›ˆë™', 'ìš´ë™', text).strip()
          text = re.sub(r'ëŒœ', 'ë‹¤', text).strip()
          text = re.sub(r'ëŒ•ê²¨', 'ë‹¤ë…€', text).strip()
          text = re.sub(r'ì—”ê°„', 'ì›¬ë§Œ', text).strip()
          text = re.sub(r'ì—‡', 'ì—ˆ', text).strip()
          text = re.sub(r'ê·¼ëŒ€', 'ê·¼ë°', text).strip()
          text = re.sub(r'ë¯„', 'ëŠ”', text).strip()
          text = re.sub(r'ëº ', 'ëº„', text).strip()
          text = re.sub(r'í•˜ì¥¬', 'í•˜ì£ ', text).strip()
          text = re.sub(r'ì‹œë¡œ', 'ì‹«ì–´', text).strip()
          if text:
            preprocessed_text.append(text)
        return preprocessed_text



    # í•¨ìˆ˜ : ë¶ˆìš©ì–´ ì œê±° (ì±„íƒ)
    def delete_stopwords(self, context):
        stopwords =  ['ã„±',
                      'ã„´',
                      'ã„·',
                      'ã„¹',
                      'ã…',
                      'ã…‚',
                      'ã……',
                      'ã…‡',
                      'ã…ˆ',
                      'ã…Š',
                      'ã…‹',
                      'ã…Œ',
                      'ã…',
                      'ã…',
                      'ã…',
                      'ã…‘',
                      'ã…“',
                      'ã…•',
                      'ã…—',
                      'ã…›',
                      'ã…œ',
                      'ã… ',
                      'ã…¡',
                      'ã…£',
                      'ğŸ˜‚',
                      'ğŸ¥º',
                      'ğŸ’•',
                      'ğŸ§¸',
                      'ğŸ’¡',
                      'ğŸ˜‰',
                      'ğŸ¥°',
                      'ğŸ˜…',
                      '\U0001fa76',
                      'ğŸ˜‹',
                      '\U0001faf0',
                      'ğŸ˜„',
                      'ğŸ¤”',
                      'ğŸ™‰',
                      '\U0001fa77',
                      'ğŸ’š',
                      'ğŸš›',
                      'ğŸ˜˜',
                      'ğŸ‘',
                      'ğŸ˜†',
                      'ğŸ˜',
                      'ğŸ¥¶',
                      'ğŸ’œ',
                      'ğŸ˜',
                      'ğŸ¤—',
                      'ğŸ‘‰',
                      'ğŸ”¥',
                      'ğŸ˜€',
                      'ğŸ˜Š',
                      'ğŸ˜™',
                      'ğŸ’“',
                      'ğŸ™ƒ',
                      'ğŸ˜­',
                      'ğŸ’',
                      'ğŸ˜ƒ',
                      'ğŸ¥²',
                      'ğŸŒ€',
                      'ğŸ¤§',
                      'ğŸ’—',
                      '\U0001fae0',
                      'ğŸ˜',
                      'ğŸ’–',
                      '\U0001fae8',
                      'ğŸ¼',
                      'ğŸ˜',
                      'ğŸ™',
                      '\U0001f979',
                      'ğŸ¥µ',
                      'ğŸ¤',
                      'ğŸ€',
                      'ğŸ’™',
                      'ğŸ˜¹',
                      'ğŸ¤­',
                      'ğŸ¤£',
                      'ğŸ¾',
                      'ğŸ¤¤',
                      'ğŸ‘€',
                      'ğŸ™‚',
                      '\U0001fa75',
                      'ğŸ»',
                      'ğŸ˜»',
                      '\U0001fae3',
                      'ğŸ˜½',
                      'ğŸ–¤',
                      '\U0001faf6',
                      'ğŸ‘',
                      'â­ï¸',
                      'âœ…',
                      '&',
                      'ğŸ¥¦',
                      'ğŸ¤',
                      'ğŸ’›',
                      'ğŸ¤',
                      '\U0001faf6ğŸ»',
                      'ğŸ»',
                      'ã„²',
                      'ã„¸',
                      'ã…ƒ',
                      'ã…†',
                      'ã…‰',
                      'ã…',
                      'ã…”',
                      'ã…’',
                      'ã…–',
                      'ã…˜',
                      'ã…š',
                      'ã…™',
                      'ã…',
                      'ã…Ÿ',
                      'ã…',
                      'ã…¢',
                      # íŠ¹ìˆ˜ë¬¸ì
                      '!', '"', '#', '$', '%', '&', "'", '(', ')', '*', '+', '-', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\', ']', '^', '_', '`', '{', '|', '}', '~',
                      # 1ì°¨
                      'â— ', 'â€¿', 'â—œ', 'á´—', 'Â´', 'ã†…', 'ã‚', 'à´¦àµà´¦à´¿', 'Ë¶', 'Ë™', 'áµ•', 'Â´', 'à¼àº¶',
                      'Ù¹', 'âŒ“', '.', '<', 'ÊºÌ¤', 'â™¡', 'åŒ—', 'â¸Œ', 'â˜»', 'â¸', 'â€¼', 'â—¡Ì', 'à¼¡',
                      'Ëƒ', 'ê‡´', 'Ë‚', 'à¹‘', 'ê‰‚', 'â—Ÿ', 'Â´', 'â—¦', 'Ï‰', 'â°', 'áŸ', 'â‘‰', 'ê’¦àº´', ';',
                      'â˜†', 'â€¢', 'á†¢'

                      ]

        preprocessed_text = []
        for text in context:
            for stopword in stopwords:
                text = text.replace(stopword, '')
            preprocessed_text.append(text)
        return preprocessed_text



    # í•¨ìˆ˜ : HTML íƒœê·¸ë¥¼ ì œê±°
    def delete_html_tag(self, context):
      preprcessed_text = []
      for text in context:
          text = re.sub(r"<[^>]+>\s+(?=<)|<[^>]+>", "", text).strip()
          if text:
              preprcessed_text.append(text)
      return preprcessed_text



    # í•¨ìˆ˜ : ì´ë©”ì¼ ì œê±°
    def delete_email(self, context):
      preprocessed_text = []
      for text in context:
        text = re.sub(r"[a-zA-Z0-9+-_.]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", "", text).strip()
        if text:
          preprocessed_text.append(text)
      return preprocessed_text



    # í•¨ìˆ˜ : í•´ì‹œíƒœê·¸(#) ì œê±° (ì±„íƒ)
    def delete_hashtag(self, context):
      preprocessed_text = []
      for text in context:
        text = re.sub(r"#\S+", "", text).strip()
        if text:
          preprocessed_text.append(text)
      return preprocessed_text



    # í•¨ìˆ˜ : ì´ëª¨í‹°ì½˜ ì œê±° (ì±„íƒ)
    def delete_emoticons(self, context):
      preprocessed_text = []
      for text in context:
        text = re.sub(r'[\U00010000-\U0010ffff]', "", text).strip()
        if text:
          preprocessed_text.append(text)
      return preprocessed_text



# ì •ê·œí™”

    # í•¨ìˆ˜ : ë°˜ë³µ íšŸìˆ˜ ì •ê·œí™” (ì±„íƒ)
    def repeat_char_normalizer(self, context):
        normalized_text = []
        for text in context:
            text = repeat_normalize(text, num_repeats=2).strip()
            if text:
                normalized_text.append(text)
        return normalized_text



    # í•¨ìˆ˜ : ë„ì–´ì“°ê¸° ì •ê·œí™” (ì±„íƒ)
    def repeated_spacing_normalizer(self, context):
        normalized_text = []
        for text in context:
            text = re.sub(r"\s+", " ", text).strip()
            if text:
                normalized_text.append(text)
        return normalized_text



    # í•¨ìˆ˜ : ì¤‘ë³µ ë¬¸ì¥ ì •ê·œí™”
    def duplicated_sentence_normalizer(self, context):
        from collections import OrderedDict
        context = list(OrderedDict.fromkeys(context))
        return context



    # í•¨ìˆ˜ : ë¬¸ì¥ì„ ìµœëŒ€, ìµœì†Œ ê¸¸ì´ë¡œ í•„í„°ë§í•˜ëŠ” ì •ê·œí™”
    def min_max_filter(self, context):
        preprocessed_text = []
        for text in context:
          if 0 < len(text) and len(text) < 250: # ë¬¸ì¥ ìµœëŒ€, ìµœì†Œ ê¸¸ì´ ì§€ì •í•´ì£¼ê¸°
            preprocessed_text.append(text)
        return preprocessed_text



# ì „ì²˜ë¦¬ í•¨ìˆ˜

    # í•¨ìˆ˜ : ì²«ë²ˆì§¸ ì „ì²˜ë¦¬
    def preprocess_first(self, context):
        context = list(context)
        # ì •ì œ
        context1 = self.delete_welcome(context)
        context2 = self.delete_stopwords(context1)
        context3 = self.delete_emoticons(context2)
        context4 = self.delete_hashtag(context3)
        # ì •ê·œí™”
        context5 = self.repeat_char_normalizer(context4)
        context6 = self.repeated_spacing_normalizer(context5)

        return pd.Series(context6)



    # í•¨ìˆ˜ : ë‘ë²ˆì§¸ ì „ì²˜ë¦¬
    def preprocess_second(self, context):

        return context


