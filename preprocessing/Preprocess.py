# -*- coding: utf-8 -*-
"""Preprocess

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XYsTYZT4KkqTk2KkfCKZxtpdlJea0iAX
"""

import requests
import json
import time
import sys
from collections import OrderedDict, namedtuple
import xml.etree.ElementTree as ET
import re
from urllib import parse
import pandas as pd
import urllib.request

"""## ë„¤ì´ë²„ ë§ì¶¤ë²• ì½”ë“œ

"""

# ì¡°ì‚¬ì™€ ì–´ë¯¸ë„ ë‹¨ì–´ë¡œ ì²˜ë¦¬í•¨. ë§ˆë•…í•œ ì˜ë‹¨ì–´ê°€ ìƒê°ì´ ì•ˆ ë‚˜ì„œ..
_checked = namedtuple('Checked',
    ['result', 'original', 'checked', 'errors', 'words', 'time'])


class Checked(_checked):
    def __new__(cls, result=False, original='', checked='', errors=0, words=[], time=0.0):
        return super(Checked, cls).__new__(
            cls, result, original, checked, errors, words, time)

    def as_dict(self):
        d = {
            'result': self.result,
            'original': self.original,
            'checked': self.checked,
            'errors': self.errors,
            'words': self.words,
            'time': self.time,
        }
        return d

    def only_checked(self):
        return self.checked

class CheckResult:
    PASSED = 0
    WRONG_SPELLING = 1
    WRONG_SPACING = 2
    AMBIGUOUS = 3
    STATISTICAL_CORRECTION = 4

base_url = 'https://m.search.naver.com/p/csearch/ocontent/util/SpellerProxy'

_agent = requests.Session()
PY3 = sys.version_info[0] == 3

def read_token():
    try :
      with open("token.txt", "r") as f:
          TOKEN = f.read()
      return TOKEN
    except :
      return "-"

def update_token(agent):
    """update passportkey
    from https://gist.github.com/AcrylicShrimp/4c94db38b7d2c4dd2e832a7d53654e42
    """

    html = agent.get(url='https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=ë§ì¶¤ë²•ê²€ì‚¬ê¸°')

    match = re.search('passportKey=([a-zA-Z0-9]+)', html.text)
    if match is not None:
        TOKEN = parse.unquote(match.group(1))
        with open("token.txt", "w") as f:
            f.write(TOKEN)

    return TOKEN

def _remove_tags(text):
    text = u"<content>{}</content>".format(text).replace("<br>","\n")
    if not PY3:
      text = text.encode("utf-8")

    result = "".join(ET.fromstring(text).itertext())

    return result

def _get_data(text, token):
    payload = {
        "_callback": "window.__jindo2_callback._spellingCheck_0",
        "q": text,
        "color_blindness": 0,
        "passportKey": token
    }
    headers = {
        "Host": "m.search.naver.com",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.9 Safari/537.36",
        "referer": "https://search.naver.com/",
        "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
        "Accept": "*/*"
    }
    start_time = time.time()
    r = _agent.get(base_url, params=payload, headers=headers)
    passed_time = time.time() - start_time
    r = r.text[42:-2]
    data = json.loads(r)
    return passed_time, data

def naver_covert_spelling(text):
    """
    ë§¤ê°œë³€ìˆ˜ë¡œ ì…ë ¥ë°›ì€ í•œê¸€ ë¬¸ì¥ì˜ ë§ì¶¤ë²•ì„ ì²´í¬í•©ë‹ˆë‹¤.
    """
    if isinstance(text, list):
        result = []
        for item in text:
            checked = naver_covert_spelling(item)
            result.append(checked)
        return result

    # ìµœëŒ€ 500ìê¹Œì§€ ê°€ëŠ¥.
    if len(text) > 500:
        return Checked(result=False)

    TOKEN = read_token()
    passed_time, data = _get_data(text, TOKEN)
    if "error" in data["message"].keys():
        TOKEN = update_token(_agent)
        passed_time, data = _get_data(text, TOKEN)
        if "error" in data["message"].keys():
            return Checked(result=False)
    html = data["message"]["result"]["html"]
    result = {
        "result": True,
        "original": text,
        "checked": _remove_tags(html),
        "errors": data["message"]["result"]["errata_count"],
        "time": passed_time,
        "words": OrderedDict(),
    }

    # ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„í•˜ê¸° ìœ„í•´ íƒœê·¸ëŠ” ì¼ë‹¨ ë³´ê¸° ì‰½ê²Œ ë°”ê¿”ë‘ .
    # ElementTreeì˜ iter()ë¥¼ ì¨ì„œ ë” ì¢‹ê²Œ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆì§€ë§Œ
    # ì´ ì§§ì€ ì½”ë“œì— êµ³ì´ ê·¸ë ‡ê²Œ í•  í•„ìš”ì„±ì´ ì—†ìœ¼ë¯€ë¡œ ì¼ë‹¨ ë¬¸ìì—´ì„ ì¹˜í™˜í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì‘ì„±.
    html = html.replace("<span class=\"green_text\">", "<green>") \
               .replace("<span class=\"red_text\">", "<red>") \
               .replace("<span class=\"purple_text\">", "<purple>") \
               .replace("<span class=\"blue_text\">", "<blue>") \
               .replace("</span>", "<end>")
    items = html.split(" ")
    words = []
    tmp = ""
    for word in items:
        if tmp == "" and word[:1] == "<":
            pos = word.find(">") + 1
            tmp = word[:pos]
        elif tmp != "":
            word = u"{}{}".format(tmp, word)

        if word[-5:] == "<end>":
            word = word.replace("<end>", "")
            tmp = ""

        words.append(word)

    for word in words:
        check_result = CheckResult.PASSED
        if word[:5] == "<red>":
            check_result = CheckResult.WRONG_SPELLING
            word = word.replace("<red>", "")
        elif word[:7] == "<green>":
            check_result = CheckResult.WRONG_SPACING
            word = word.replace("<green>", "")
        elif word[:8] == "<purple>":
            check_result = CheckResult.AMBIGUOUS
            word = word.replace("<purple>", "")
        elif word[:6] == "<blue>":
            check_result = CheckResult.STATISTICAL_CORRECTION
            word = word.replace("<blue>", "")
        result["words"][word] = check_result

    result = Checked(**result)

    return result.checked

"""## ë¶€ì‚°ëŒ€ ë§ì¶¤ë²• ì½”ë“œ

"""

def busan_covert_spelling(text):
  response = requests.post('http://164.125.7.61/speller/results', data={'text1': text})
  data = response.text.split('data = [', 1)[-1].rsplit('];', 1)[0]
  # JSON ë””ì½”ë”© ì‹œë„
  data = json.loads(data)

  # ì´í›„ì— ìˆ˜í–‰í•  ì‘ì—… ì¶”ê°€
  for error_text in data['errInfo']:
    err_word = error_text["orgStr"]
    corr_word = error_text["candWord"].split("|")[0]
    text = text.replace(err_word,corr_word)

  return text


from soynlp.normalizer import repeat_normalize

"""*ê¸°íƒ€ ì „ì²˜ë¦¬*"""

my_stopwords = []

# ë¶ˆìš©ì–´ ì‚¬ì „ì— í•œêµ­ì–´ ììŒ, ëª¨ìŒ ì¶”ê°€
korean_consonants = "ã„±ã„´ã„·ã„¹ã…ã…‚ã……ã…‡ã…ˆã…Šã…‹ã…Œã…ã…"
korean_vowels = "ã…ã…‘ã…“ã…•ã…—ã…›ã…œã… ã…¡ã…£"

my_stopwords.extend(korean_consonants)
my_stopwords.extend(korean_vowels)

my_stopwords.extend(['ğŸ˜‚',
 'ğŸ¥º',
 'ğŸ’•',
 'ğŸ§¸',
 'ğŸ’¡',
 'ğŸ˜‰',
 'ğŸ¥°',
 'ğŸ˜…',
 '\U0001fa76',
 'ğŸ˜‹',
 '\U0001faf0',
 'ğŸ˜„',
 'ğŸ¤”',
 'ğŸ™‰',
 '\U0001fa77',
 'ğŸ’š',
 'ğŸš›',
 'ğŸ˜˜',
 'ğŸ‘',
 'ğŸ˜†',
 'ğŸ˜',
 'ğŸ¥¶',
 'ğŸ’œ',
 'ğŸ˜',
 'ğŸ¤—',
 'ğŸ‘‰',
 'ğŸ”¥',
 'ğŸ˜€',
 'ğŸ˜Š',
 'ğŸ˜™',
 'ğŸ’“',
 'ğŸ™ƒ',
 'ğŸ˜­',
 'ğŸ’',
 'ğŸ˜ƒ',
 'ğŸ¥²',
 'ğŸŒ€',
 'ğŸ¤§',
 'ğŸ’—',
 '\U0001fae0',
 'ğŸ˜',
 'ğŸ’–',
 '\U0001fae8',
 'ğŸ¼',
 'ğŸ˜',
 'ğŸ™',
 '\U0001f979',
 'ğŸ¥µ',
 'ğŸ¤',
 'ğŸ€',
 'ğŸ’™',
 'ğŸ˜¹',
 'ğŸ¤­',
 'ğŸ¤£',
 'ğŸ¾',
 'ğŸ¤¤',
 'ğŸ‘€',
 'ğŸ™‚',
 '\U0001fa75',
 'ğŸ»',
 'ğŸ˜»',
 '\U0001fae3',
 'ğŸ˜½',
 'ğŸ–¤',
 '\U0001faf6',
 'ğŸ‘',
 'â­ï¸',
 'âœ…',
 '&'])

my_stopwords = list(set(my_stopwords))
my_stopwords



class NLP_Preprocessor:

    # í•¨ìˆ˜ : ìˆ«ìëŠ” [number]ë¡œ ë³€í™˜ +  + ë¶ˆìš©ì–´ ì œê±°
    def delete_stopwords(self, context):

        context = re.sub(r'ã„±ã…Š', 'ê´œì°®', context)
        context = re.sub(r'\d+', '[number]', context)

        for stopword in my_stopwords:
            context = context.replace(stopword, '')
        return context


    # í•¨ìˆ˜ : ìœ„ 4ê°œì˜ í•¨ìˆ˜ í•œë²ˆì— ì‹¤í–‰
    def preprocess(self, context):
        context = re.sub(r'[-=+#/\?:^@*\"â€»~ã†!ã€â€˜|>\(\)\[\]`\'â€¦ã€‹\â€\â€œ\â€™Â·]',' ',context)
        context = re.sub(r'ã„±ã…Š', 'ê´œì°®', context)
        context = re.sub(r'\d+', '[number]', context)
        context = re.sub(r'S|M|L|XL|XXL|s|m|l|xl|xxl|ìŠ¤ëª°|ë¯¸ë””ì›€|ë¼ì§€|ì—‘ìŠ¤ë¼ì§€|ë¯¸ë“|ì—‘ë¼','[size]',context)

        context = self.delete_stopwords(context)

        context = repeat_normalize(context, num_repeats=2).strip()
        context = re.sub(r"\s+", " ", context).strip()
        return context

